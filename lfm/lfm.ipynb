{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 先定义一个评分矩阵"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[5, 2, 0, 0, 1],\n",
       "       [1, 0, 3, 0, 2],\n",
       "       [0, 3, 5, 3, 0],\n",
       "       [1, 1, 1, 0, 0],\n",
       "       [0, 3, 4, 0, 0]])"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "R = np.array([\n",
    "         [5,2,0,0,1],\n",
    "         [1,0,3,0,2],\n",
    "         [0,3,5,3,0],\n",
    "         [1,1,1,0,0],\n",
    "         [0,3,4,0,0],\n",
    "])\n",
    "R"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 我们的任务就是把这个评分矩阵，给分解成两个小矩阵， R = PQ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 定义一个lfm函数\n",
    "\n",
    "# R: 评分矩阵\n",
    "# K：特征维度\n",
    "# alpha： 学习率\n",
    "# lanbda： 正则化系数\n",
    "# epoch: 梯度下降的次数\n",
    "\n",
    "def lfm(R,K,alpha,lanbda,epoch):\n",
    "    M = R.shape[0]\n",
    "    N = R.shape[1]\n",
    "    \n",
    "    P = np.random.rand(M,K)\n",
    "    Q = np.random.rand(K,N)\n",
    "    \n",
    "    loss_arr = []\n",
    "    \n",
    "    for step in range(epoch):\n",
    "        # 写梯度下降的方法\n",
    "        for u in range(M):\n",
    "            for i in range(N):\n",
    "\n",
    "                if R[u][i] > 0: #有评分的时候，才进行迭代\n",
    "\n",
    "                    # 先算出（）里面的\n",
    "                    eui = np.dot(P[u,:],Q[:,i]) - R[u][i]\n",
    "\n",
    "                    for k in range(K):\n",
    "                        P[u,k] = P[u,k] - alpha * (eui * Q[k][i])\n",
    "                        Q[k,i] = Q[k,i] - alpha * (eui * P[u][k])\n",
    "        \n",
    "        # 让我们来计算loss\n",
    "        loss = 0 \n",
    "        for u in range(M):\n",
    "            for i in  range(N):\n",
    "                if R[u][i] > 0 :\n",
    "                    loss += (np.dot(P[u,:],Q[:,i])-R[u][i])**2\n",
    "        \n",
    "        if(step % 3 == 0):\n",
    "            print(\"step=\",step,\",loss=\",loss)\n",
    "        loss_arr.append(loss)\n",
    "            \n",
    "    return P,Q,loss_arr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step= 0 ,loss= 89.69736432238639\n",
      "step= 3 ,loss= 76.97801810560361\n",
      "step= 6 ,loss= 62.412229717667046\n",
      "step= 9 ,loss= 47.126171111482584\n",
      "step= 12 ,loss= 33.08262570291139\n",
      "step= 15 ,loss= 22.050444489962704\n",
      "step= 18 ,loss= 14.606005494192285\n",
      "step= 21 ,loss= 10.137139182086585\n",
      "step= 24 ,loss= 7.605774755542006\n",
      "step= 27 ,loss= 6.159409895401741\n",
      "step= 30 ,loss= 5.280718969470162\n",
      "step= 33 ,loss= 4.699618512486768\n",
      "step= 36 ,loss= 4.282122255830442\n",
      "step= 39 ,loss= 3.9603535297222945\n",
      "step= 42 ,loss= 3.697705952806455\n",
      "step= 45 ,loss= 3.4730117323880965\n",
      "step= 48 ,loss= 3.273383341359534\n",
      "step= 51 ,loss= 3.0907835881964005\n",
      "step= 54 ,loss= 2.920216655237524\n",
      "step= 57 ,loss= 2.7586720006834295\n",
      "step= 60 ,loss= 2.6044546096977266\n",
      "step= 63 ,loss= 2.4567352362164345\n",
      "step= 66 ,loss= 2.315237187933368\n",
      "step= 69 ,loss= 2.180013539156749\n",
      "step= 72 ,loss= 2.051287387101596\n",
      "step= 75 ,loss= 1.929338100660243\n",
      "step= 78 ,loss= 1.8144225143823627\n",
      "step= 81 ,loss= 1.7067235189185745\n",
      "step= 84 ,loss= 1.606320466906447\n",
      "step= 87 ,loss= 1.5131768639093934\n",
      "step= 90 ,loss= 1.4271413694689763\n",
      "step= 93 ,loss= 1.3479584823284796\n",
      "step= 96 ,loss= 1.2752856069070766\n",
      "step= 99 ,loss= 1.208713586147341\n",
      "step= 102 ,loss= 1.1477882613506492\n",
      "step= 105 ,loss= 1.0920311596508236\n",
      "step= 108 ,loss= 1.0409579687822594\n",
      "step= 111 ,loss= 0.9940939873564327\n",
      "step= 114 ,loss= 0.9509861965794907\n",
      "step= 117 ,loss= 0.9112119613204036\n",
      "step= 120 ,loss= 0.8743846262622847\n",
      "step= 123 ,loss= 0.8401564319296122\n",
      "step= 126 ,loss= 0.8082192503947455\n",
      "step= 129 ,loss= 0.7783036507589852\n",
      "step= 132 ,loss= 0.750176770096855\n",
      "step= 135 ,loss= 0.7236394044524708\n",
      "step= 138 ,loss= 0.6985226612124664\n",
      "step= 141 ,loss= 0.6746844393552072\n",
      "step= 144 ,loss= 0.6520059346141494\n",
      "step= 147 ,loss= 0.6303883063583773\n",
      "step= 150 ,loss= 0.6097495935493993\n",
      "step= 153 ,loss= 0.5900219284745662\n",
      "step= 156 ,loss= 0.5711490681275562\n",
      "step= 159 ,loss= 0.5530842426693477\n",
      "step= 162 ,loss= 0.5357883067735402\n",
      "step= 165 ,loss= 0.5192281713102721\n",
      "step= 168 ,loss= 0.5033754883989443\n",
      "step= 171 ,loss= 0.4882055612294687\n",
      "step= 174 ,loss= 0.4736964503119655\n",
      "step= 177 ,loss= 0.4598282492743206\n",
      "step= 180 ,loss= 0.4465825054764765\n",
      "step= 183 ,loss= 0.43394176318861394\n",
      "step= 186 ,loss= 0.42188920964292254\n",
      "step= 189 ,loss= 0.4104084067583883\n",
      "step= 192 ,loss= 0.39948309366196944\n",
      "step= 195 ,loss= 0.3890970472395526\n",
      "step= 198 ,loss= 0.3792339898284972\n",
      "step= 201 ,loss= 0.3698775348120499\n",
      "step= 204 ,loss= 0.36101116230783276\n",
      "step= 207 ,loss= 0.35261821837746044\n",
      "step= 210 ,loss= 0.3446819322441476\n",
      "step= 213 ,loss= 0.3371854469120317\n",
      "step= 216 ,loss= 0.3301118593558456\n",
      "step= 219 ,loss= 0.3234442671113242\n",
      "step= 222 ,loss= 0.31716581866204996\n",
      "step= 225 ,loss= 0.3112597655015398\n",
      "step= 228 ,loss= 0.3057095141624145\n",
      "step= 231 ,loss= 0.3004986768575993\n",
      "step= 234 ,loss= 0.2956111196801701\n",
      "step= 237 ,loss= 0.2910310075657185\n",
      "step= 240 ,loss= 0.28674284543986245\n",
      "step= 243 ,loss= 0.28273151515866535\n",
      "step= 246 ,loss= 0.2789823080054821\n",
      "step= 249 ,loss= 0.27548095263762334\n",
      "step= 252 ,loss= 0.27221363848337915\n",
      "step= 255 ,loss= 0.2691670346770411\n",
      "step= 258 ,loss= 0.2663283046890741\n",
      "step= 261 ,loss= 0.2636851168626115\n",
      "step= 264 ,loss= 0.2612256511080554\n",
      "step= 267 ,loss= 0.258938602036447\n",
      "step= 270 ,loss= 0.25681317883111265\n",
      "step= 273 ,loss= 0.25483910216740946\n",
      "step= 276 ,loss= 0.25300659849346946\n",
      "step= 279 ,loss= 0.25130639198200555\n",
      "step= 282 ,loss= 0.2497296944555161\n",
      "step= 285 ,loss= 0.24826819357568175\n",
      "step= 288 ,loss= 0.24691403957320562\n",
      "step= 291 ,loss= 0.24565983077764586\n",
      "step= 294 ,loss= 0.24449859818855824\n",
      "step= 297 ,loss= 0.24342378931009784\n",
      "step= 300 ,loss= 0.2424292514516577\n",
      "step= 303 ,loss= 0.24150921467747846\n",
      "step= 306 ,loss= 0.24065827456888228\n",
      "step= 309 ,loss= 0.23987137494404484\n",
      "step= 312 ,loss= 0.23914379066231012\n",
      "step= 315 ,loss= 0.23847111062307205\n",
      "step= 318 ,loss= 0.23784922105333484\n",
      "step= 321 ,loss= 0.23727428916331736\n",
      "step= 324 ,loss= 0.2367427472358447\n",
      "step= 327 ,loss= 0.23625127720289346\n",
      "step= 330 ,loss= 0.2357967957514165\n",
      "step= 333 ,loss= 0.2353764399905058\n",
      "step= 336 ,loss= 0.2349875537029708\n",
      "step= 339 ,loss= 0.23462767419649522\n",
      "step= 342 ,loss= 0.23429451976260324\n",
      "step= 345 ,loss= 0.23398597774564556\n",
      "step= 348 ,loss= 0.23370009321887122\n",
      "step= 351 ,loss= 0.23343505826026642\n",
      "step= 354 ,loss= 0.23318920181717492\n",
      "step= 357 ,loss= 0.2329609801457031\n",
      "step= 360 ,loss= 0.23274896780845336\n",
      "step= 363 ,loss= 0.2325518492122156\n",
      "step= 366 ,loss= 0.23236841066575653\n",
      "step= 369 ,loss= 0.23219753293677398\n",
      "step= 372 ,loss= 0.23203818428635797\n",
      "step= 375 ,loss= 0.23188941395886278\n",
      "step= 378 ,loss= 0.23175034610493217\n",
      "step= 381 ,loss= 0.23162017411545874\n",
      "step= 384 ,loss= 0.23149815534449408\n",
      "step= 387 ,loss= 0.2313836061995046\n",
      "step= 390 ,loss= 0.23127589757787428\n",
      "step= 393 ,loss= 0.23117445062916508\n",
      "step= 396 ,loss= 0.23107873282332048\n",
      "step= 399 ,loss= 0.23098825430574052\n",
      "step= 402 ,loss= 0.23090256452094968\n",
      "step= 405 ,loss= 0.2308212490873654\n",
      "step= 408 ,loss= 0.23074392690653892\n",
      "step= 411 ,loss= 0.2306702474910286\n",
      "step= 414 ,loss= 0.23059988849593474\n",
      "step= 417 ,loss= 0.2305325534399188\n",
      "step= 420 ,loss= 0.23046796960234825\n",
      "step= 423 ,loss= 0.23040588608398535\n",
      "step= 426 ,loss= 0.23034607201939378\n",
      "step= 429 ,loss= 0.23028831492999255\n",
      "step= 432 ,loss= 0.23023241920735407\n",
      "step= 435 ,loss= 0.23017820471705003\n",
      "step= 438 ,loss= 0.2301255055139805\n",
      "step= 441 ,loss= 0.23007416866072514\n",
      "step= 444 ,loss= 0.23002405314104224\n",
      "step= 447 ,loss= 0.22997502886119484\n",
      "step= 450 ,loss= 0.2299269757322918\n",
      "step= 453 ,loss= 0.22987978282731475\n",
      "step= 456 ,loss= 0.22983334760697832\n",
      "step= 459 ,loss= 0.22978757520898066\n",
      "step= 462 ,loss= 0.22974237779561044\n",
      "step= 465 ,loss= 0.2296976739550593\n",
      "step= 468 ,loss= 0.22965338815212719\n",
      "step= 471 ,loss= 0.2296094502243488\n",
      "step= 474 ,loss= 0.22956579491985993\n",
      "step= 477 ,loss= 0.22952236147361935\n",
      "step= 480 ,loss= 0.229479093218859\n",
      "step= 483 ,loss= 0.22943593723087627\n",
      "step= 486 ,loss= 0.22939284400052112\n",
      "step= 489 ,loss= 0.2293497671349218\n",
      "step= 492 ,loss= 0.22930666308321665\n",
      "step= 495 ,loss= 0.2292634908851999\n",
      "step= 498 ,loss= 0.22922021194099818\n",
      "step= 501 ,loss= 0.22917678980001208\n",
      "step= 504 ,loss= 0.22913318996753113\n",
      "step= 507 ,loss= 0.22908937972752363\n",
      "step= 510 ,loss= 0.22904532798026886\n",
      "step= 513 ,loss= 0.22900100509356916\n",
      "step= 516 ,loss= 0.22895638276640728\n",
      "step= 519 ,loss= 0.22891143390400062\n",
      "step= 522 ,loss= 0.22886613250328708\n",
      "step= 525 ,loss= 0.22882045354796768\n",
      "step= 528 ,loss= 0.22877437291229158\n",
      "step= 531 ,loss= 0.22872786727284894\n",
      "step= 534 ,loss= 0.22868091402768612\n",
      "step= 537 ,loss= 0.22863349122213186\n",
      "step= 540 ,loss= 0.2285855774807517\n",
      "step= 543 ,loss= 0.2285371519449279\n",
      "step= 546 ,loss= 0.22848819421556602\n",
      "step= 549 ,loss= 0.22843868430051212\n",
      "step= 552 ,loss= 0.22838860256626578\n",
      "step= 555 ,loss= 0.22833792969363145\n",
      "step= 558 ,loss= 0.2282866466369739\n",
      "step= 561 ,loss= 0.2282347345867632\n",
      "step= 564 ,loss= 0.2281821749351445\n",
      "step= 567 ,loss= 0.2281289492442596\n",
      "step= 570 ,loss= 0.22807503921710232\n",
      "step= 573 ,loss= 0.228020426670682\n",
      "step= 576 ,loss= 0.22796509351130956\n",
      "step= 579 ,loss= 0.2279090217118163\n",
      "step= 582 ,loss= 0.22785219329055728\n",
      "step= 585 ,loss= 0.22779459029203578\n",
      "step= 588 ,loss= 0.2277361947690247\n",
      "step= 591 ,loss= 0.22767698876605105\n",
      "step= 594 ,loss= 0.22761695430414186\n",
      "step= 597 ,loss= 0.2275560733667129\n",
      "step= 600 ,loss= 0.22749432788652355\n",
      "step= 603 ,loss= 0.22743169973359895\n",
      "step= 606 ,loss= 0.22736817070404486\n",
      "step= 609 ,loss= 0.2273037225096864\n",
      "step= 612 ,loss= 0.22723833676846464\n",
      "step= 615 ,loss= 0.22717199499552979\n",
      "step= 618 ,loss= 0.22710467859498001\n",
      "step= 621 ,loss= 0.22703636885219447\n",
      "step= 624 ,loss= 0.22696704692671957\n",
      "step= 627 ,loss= 0.2268966938456647\n",
      "step= 630 ,loss= 0.22682529049757427\n",
      "step= 633 ,loss= 0.22675281762673832\n",
      "step= 636 ,loss= 0.22667925582791731\n",
      "step= 639 ,loss= 0.22660458554144985\n",
      "step= 642 ,loss= 0.2265287870487222\n",
      "step= 645 ,loss= 0.2264518404679712\n",
      "step= 648 ,loss= 0.2263737257504116\n",
      "step= 651 ,loss= 0.22629442267665947\n",
      "step= 654 ,loss= 0.22621391085344109\n",
      "step= 657 ,loss= 0.2261321697105726\n",
      "step= 660 ,loss= 0.2260491784981983\n",
      "step= 663 ,loss= 0.22596491628427234\n",
      "step= 666 ,loss= 0.22587936195228056\n",
      "step= 669 ,loss= 0.2257924941991895\n",
      "step= 672 ,loss= 0.22570429153361396\n",
      "step= 675 ,loss= 0.22561473227420148\n",
      "step= 678 ,loss= 0.22552379454822422\n",
      "step= 681 ,loss= 0.22543145629037317\n",
      "step= 684 ,loss= 0.2253376952417562\n",
      "step= 687 ,loss= 0.2252424889490894\n",
      "step= 690 ,loss= 0.22514581476408438\n",
      "step= 693 ,loss= 0.22504764984302977\n",
      "step= 696 ,loss= 0.224947971146562\n",
      "step= 699 ,loss= 0.22484675543962787\n",
      "step= 702 ,loss= 0.2247439792916385\n",
      "step= 705 ,loss= 0.22463961907681426\n",
      "step= 708 ,loss= 0.22453365097472008\n",
      "step= 711 ,loss= 0.22442605097099572\n",
      "step= 714 ,loss= 0.2243167948582791\n",
      "step= 717 ,loss= 0.22420585823732567\n",
      "step= 720 ,loss= 0.22409321651832972\n",
      "step= 723 ,loss= 0.22397884492243902\n",
      "step= 726 ,loss= 0.22386271848348174\n",
      "step= 729 ,loss= 0.22374481204989394\n",
      "step= 732 ,loss= 0.22362510028685978\n",
      "step= 735 ,loss= 0.22350355767866303\n",
      "step= 738 ,loss= 0.22338015853125637\n",
      "step= 741 ,loss= 0.22325487697505075\n",
      "step= 744 ,loss= 0.22312768696793092\n",
      "step= 747 ,loss= 0.2229985622984965\n",
      "step= 750 ,loss= 0.22286747658953798\n",
      "step= 753 ,loss= 0.222734403301751\n",
      "step= 756 ,loss= 0.2225993157376884\n",
      "step= 759 ,loss= 0.22246218704596188\n",
      "step= 762 ,loss= 0.2223229902256957\n",
      "step= 765 ,loss= 0.22218169813123045\n",
      "step= 768 ,loss= 0.22203828347709414\n",
      "step= 771 ,loss= 0.22189271884323367\n",
      "step= 774 ,loss= 0.22174497668051854\n",
      "step= 777 ,loss= 0.22159502931651998\n",
      "step= 780 ,loss= 0.22144284896156954\n",
      "step= 783 ,loss= 0.22128840771510258\n",
      "step= 786 ,loss= 0.22113167757229213\n",
      "step= 789 ,loss= 0.22097263043097812\n",
      "step= 792 ,loss= 0.22081123809889575\n",
      "step= 795 ,loss= 0.22064747230120957\n",
      "step= 798 ,loss= 0.22048130468835386\n",
      "step= 801 ,loss= 0.22031270684419355\n",
      "step= 804 ,loss= 0.2201416502944975\n",
      "step= 807 ,loss= 0.2199681065157385\n",
      "step= 810 ,loss= 0.21979204694422128\n",
      "step= 813 ,loss= 0.21961344298554158\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step= 816 ,loss= 0.2194322660243833\n",
      "step= 819 ,loss= 0.21924848743465317\n",
      "step= 822 ,loss= 0.21906207858996476\n",
      "step= 825 ,loss= 0.2188730108744668\n",
      "step= 828 ,loss= 0.21868125569402258\n",
      "step= 831 ,loss= 0.21848678448774678\n",
      "step= 834 ,loss= 0.21828956873990216\n",
      "step= 837 ,loss= 0.21808957999215065\n",
      "step= 840 ,loss= 0.21788678985617435\n",
      "step= 843 ,loss= 0.21768117002665907\n",
      "step= 846 ,loss= 0.21747269229464894\n",
      "step= 849 ,loss= 0.21726132856126532\n",
      "step= 852 ,loss= 0.21704705085180226\n",
      "step= 855 ,loss= 0.21682983133018946\n",
      "step= 858 ,loss= 0.2166096423138336\n",
      "step= 861 ,loss= 0.21638645628882533\n",
      "step= 864 ,loss= 0.21616024592552627\n",
      "step= 867 ,loss= 0.2159309840945246\n",
      "step= 870 ,loss= 0.2156986438829619\n",
      "step= 873 ,loss= 0.21546319861123284\n",
      "step= 876 ,loss= 0.21522462185005048\n",
      "step= 879 ,loss= 0.21498288743787755\n",
      "step= 882 ,loss= 0.2147379694987236\n",
      "step= 885 ,loss= 0.21448984246029643\n",
      "step= 888 ,loss= 0.214238481072514\n",
      "step= 891 ,loss= 0.21398386042636405\n",
      "step= 894 ,loss= 0.21372595597311045\n",
      "step= 897 ,loss= 0.21346474354383868\n",
      "step= 900 ,loss= 0.21320019936933648\n",
      "step= 903 ,loss= 0.21293230010029873\n",
      "step= 906 ,loss= 0.21266102282785254\n",
      "step= 909 ,loss= 0.2123863451043902\n",
      "step= 912 ,loss= 0.21210824496470726\n",
      "step= 915 ,loss= 0.2118267009474266\n",
      "step= 918 ,loss= 0.21154169211670623\n",
      "step= 921 ,loss= 0.2112531980842163\n",
      "step= 924 ,loss= 0.2109611990313703\n",
      "step= 927 ,loss= 0.2106656757318033\n",
      "step= 930 ,loss= 0.2103666095740795\n",
      "step= 933 ,loss= 0.21006398258461317\n",
      "step= 936 ,loss= 0.20975777745079416\n",
      "step= 939 ,loss= 0.20944797754429328\n",
      "step= 942 ,loss= 0.20913456694453741\n",
      "step= 945 ,loss= 0.20881753046233312\n",
      "step= 948 ,loss= 0.2084968536636204\n",
      "step= 951 ,loss= 0.2081725228933358\n",
      "step= 954 ,loss= 0.20784452529936948\n",
      "step= 957 ,loss= 0.20751284885658847\n",
      "step= 960 ,loss= 0.20717748239090567\n",
      "step= 963 ,loss= 0.2068384156033748\n",
      "step= 966 ,loss= 0.20649563909428523\n",
      "step= 969 ,loss= 0.20614914438722856\n",
      "step= 972 ,loss= 0.20579892395311888\n",
      "step= 975 ,loss= 0.20544497123413188\n",
      "step= 978 ,loss= 0.2050872806675426\n",
      "step= 981 ,loss= 0.20472584770942648\n",
      "step= 984 ,loss= 0.20436066885820608\n",
      "step= 987 ,loss= 0.2039917416779979\n",
      "step= 990 ,loss= 0.20361906482174544\n",
      "step= 993 ,loss= 0.20324263805409376\n",
      "step= 996 ,loss= 0.20286246227398433\n",
      "step= 999 ,loss= 0.20247853953692968\n"
     ]
    }
   ],
   "source": [
    "K = 2\n",
    "alpha = 0.01\n",
    "lanbda = 0.2\n",
    "epoch = 1000\n",
    "\n",
    "P,Q,loss_arr = lfm(R,K,alpha,lanbda,epoch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[5 2 0 0 1]\n",
      " [1 0 3 0 2]\n",
      " [0 3 5 3 0]\n",
      " [1 1 1 0 0]\n",
      " [0 3 4 0 0]]\n",
      "[[4.98914153 2.02702846 1.97099958 1.25282615 0.99547362]\n",
      " [0.99333778 1.5674229  3.0094646  1.7984505  1.99616204]\n",
      " [3.73105062 3.06647439 4.9606772  3.00066538 3.13988647]\n",
      " [1.05946452 0.73881151 1.11194298 0.6764589  0.68776929]\n",
      " [5.19038546 2.98000477 4.00953106 2.46290037 2.3815232 ]]\n"
     ]
    }
   ],
   "source": [
    "print(R)\n",
    "print(np.dot(P,Q))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x18b47c88>]"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# 打印一下loss下降\n",
    "x = np.arange(epoch)\n",
    "y = loss_arr\n",
    "import matplotlib.pyplot as plt\n",
    "plt.plot(x,y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
